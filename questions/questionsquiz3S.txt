1
Multiple Choice
While using PAPI to profile an HPC application, you observe a high number of cache misses. What are some techniques you could employ to reduce these cache misses and improve performance?

Implement more aggressive branch prediction algorithms

Increase the clock speed of the CPU

Increase the number of MPI processes

Optimize data structures for better cache locality

Use checkpointing to save frequently used data

Question 2
2
Multiple Choice
Given the I/O demands of a genomic sequencing application that generates petabytes of data, which filesystem is most appropriate, and why?

NFS for its ease of use in networked environments.

NTFS for its reliability in single-node environments.

HDFS for its redundancy and big data analytics capabilities.

Ext4 for its widespread support across Linux systems.

Lustre for its scalability and ability to manage large datasets efficiently.

Question 3
3
Multiple Choice
In a parallel filesystem like Lustre, what is the purpose of the Lustre Distributed Lock Manager (LDLM), and how does it contribute to system performance?

LDLM encrypts data before it is written to the storage devices.

LDLM is responsible for compressing data to save storage space.

LDLM handles the backup and recovery operations for the filesystem.

LDLM manages the locks for accessing resources, preventing conflicts in high-concurrency environments.

LDLM monitors network traffic and manages load balancing across nodes.

Question 4
4
Multiple Choice
In the following C++ code, you are using Intel VTune Amplifier to identify performance bottlenecks:

#include <iostream>
#include <vector>
 
void compute(std::vector<int>& data) {
   for (auto& x : data) {
       x *= 2;
   }
}
 
int main() {
   std::vector<int> data(1000000, 1);
   compute(data);
   std::cout << "Data processed." << std::endl;
   return 0;
}
VTune indicates that the compute function has poor memory access patterns. What could you do to improve this?

Parallelize the loop using OpenMP.

Use SIMD instructions to optimize the loop.

Reduce the size of data.

Replace std::vector with a raw array.

Reorganize the loop to improve cache locality.

Question 5
5
Multiple Choice
What is the primary benefit of implementing load balancing in a parallel filesystem?

It encrypts data before it is written to disk.

It ensures an even distribution of I/O workloads across all available resources.

It reduces the complexity of managing the filesystem.

It increases the storage capacity of the filesystem.

It simplifies the process of adding new storage devices to the system.

Question 6
6
Multiple Choice
What HPC software libraries will you use for partial differential equations?

PAPI, Vampir

METIS, ParMETIS 

Pthreads, MPI, Boost MPI

A.SuperLU, PETSc, SLEPc, ELPA, Hypre 

PETSc, Trilinos

Question 7
7
Multiple Choice
Performance Monitoring in a Real-World Scenario You're trying to optimize a fluid dynamics simulation running on a supercomputer. You notice that certain parts of the program run significantly slower than others. Which tool, designed for collecting performance metrics and visualizing data for profiling, would be most suitable to diagnose the issue?

HDF5 - Used for structured storage and retrieval of large datasets.

PETSc - Aims to solve PDEs on various grid types.

FFTW - Used for Discrete Fourier Transforms.

Trilinos - Another tool for solving PDEs.

PAPI - Gives users access to hardware counters to collect performance metrics.

Question 8
8
Multiple Choice
You are tasked with reducing the frequency of checkpointing in a weather simulation running on an HPC system. What factors should you consider when determining the optimal checkpoint interval?

The clock speed of the CPU and the total simulation time

The number of MPI processes and the size of the simulation grid

The precision of the simulation and the complexity of the atmospheric model

The overhead of I/O operations and the likelihood of system failures.

The number of checkpoints stored and the memory usage of the application

Question 9
9
Multiple Choice
What feature of GPFS makes it particularly well-suited for data-intensive projects like the Square Kilometre Array (SKA)?

Its integration with low-speed network infrastructures.

Its focus on local storage management.

Its ability to manage vast data volumes across global data centers.

Its simple installation and configuration process.

Its support for single-node operations.

Question 10
10
Multiple Choice
You have identified a critical section in your HPC application where multiple threads are contending for a lock, causing performance degradation. How can you address this bottleneck?

Increase the number of threads to overcome the bottleneck.

Replace the lock with atomic operations to reduce contention.

Use a semaphore instead of a lock to manage thread synchronization

Implement MPI to distribute the workload across nodes

Use more frequent checkpointing to reduce the time spent in the critical section

Question 11
11
Multiple Choice
You are profiling an HPC application using Perf and notice that a significant amount of time is spent in system calls. What could this indicate about your application, and what might be a possible optimization strategy?

Indicates excessive network communication; reduce communication overhead.

Indicates insufficient parallelism; refactor code to use more cores.

Indicates poor CPU utilization; consider increasing thread count.

Indicates frequent memory allocation; optimize memory usage or batch operations.

Indicates inefficient disk I/O; optimize file access patterns.

Question 12
12
Multiple Choice
You are profiling a C++ application using gprof. After compiling and running the program with gprof instrumentation, you receive the following output:

Flat profile:

Each sample counts as 0.01 seconds.

 %  cumulative  self             self    total          
 time  seconds  seconds   calls  ms/call ms/call name   
 60.00     0.60    0.60    1000    0.60    0.60 compute
 40.00     1.00    0.40                            main
Given this output, what would be the most effective optimization strategy?

Reduce the number of calls to compute.

Increase the sampling rate in gprof.

Optimize the compute function for better cache performance.

Inline the main function.

Use a different profiling tool with lower overhead.

Question 13
13
Multiple Choice
How do solid-state drives (SSDs) improve I/O performance in HPC storage systems compared to traditional spinning disks?

By simplifying the management of data across multiple nodes.

By reducing the need for metadata management.

By offering better data redundancy and fault tolerance.

By providing larger storage capacity at a lower cost.

By reducing latency and increasing IOPS (input/output operations per second).

Question 14
14
Multiple Choice
Mesh Decomposition in Aerospace Engineering An aerospace engineer is working on a simulation of airflow over an aircraft wing. To ensure parallel computing efficiency, the engineer needs to decompose the computational mesh around the wing. Which tool specializes in this?

SuperLU - Solves systems of linear equations.

VTK - Used for 3D data visualization.

Boost MPI - C++ interface for Message Passing Interface.

ELPA - Performs matrix operations.

METIS - Efficiently decomposes graphs and meshes for parallel computing.

Question 15
15
Multiple Choice
You are tasked with selecting a filesystem for a genomic sequencing application that will generate and process petabytes of data. Which filesystem would be most appropriate, and what feature makes it particularly suitable?

NFS for its ease of use in network environments.

Lustre for its scalability and high throughput in managing massive datasets.

HDFS for its redundancy and fault tolerance.

Ext4 for its widespread use and simplicity.

NTFS for its reliability on Windows systems.

Question 16
16
Multiple Choice
A genomics data analysis algorithm scales poorly as the number of processors increases on an HPC system. What is the most likely cause of this scalability issue?

The algorithm's time complexity is too low.

The code is written in a high-level programming language.

The algorithm has excessive inter-processor communication.

The input data is too small to benefit from parallelism.

The algorithm uses too much memory.

Question 17
17
Multiple Choice
Which data replication strategy is most effective in ensuring fault tolerance in an HPC environment?

RAID-0, which focuses on performance rather than redundancy.

Asynchronous replication where data is periodically copied to a remote site.

Multi-node replication where data is replicated across multiple nodes in real-time.

Single-point replication where data is duplicated on a single backup node.

Dynamic replication based on current I/O load.

Question 18
18
Multiple Choice
Linear Algebra in HPC Real-World Scenario Imagine you are working on a weather simulation program that requires solving a large system of linear equations. Which tool, known for providing routines to solve systems of linear equations, would you consider using?

Linear Algebra Package (LAPACK) - Provides routines for solving systems of linear equations, eigenvalue problems, and matrix inversion.

VTK - Used for 3D data rendering and visualization.

Parallel Boost Graph Library - Designed for large-scale graph operations.

Basic Linear Algebra Subprograms (BLAS) - Focuses on basic vector and matrix operations.

METIS - Used for decomposing graphs/meshes for parallel computing.

Question 19
19
Multiple Choice
In the code snippet provided, identify the correct procedure for initializing the HDF5 file for parallel I/O. Which of the following statements about the code are accurate?

#include "hdf5.h"
#include <mpi.h>
 
int main(int argc, char **argv) {
   MPI_Init(&argc, &argv);
 
   // Initialize MPI and HDF5 file access properties
   MPI_Comm comm = MPI_COMM_WORLD;
   MPI_Info info = MPI_INFO_NULL;
   hid_t plist_id = H5Pcreate(H5P_FILE_ACCESS);
   H5Pset_fapl_mpio(plist_id, comm, info);
 
   // Create and open the HDF5 file for parallel I/O
   hid_t file_id = H5Fcreate("data.h5", H5F_ACC_TRUNC, H5P_DEFAULT, plist_id);
 
   // Write data (assume data buffer exists)
   hid_t dspace_id = H5Screate_simple(1, dims, NULL); // Dataset dimensions
   hid_t dset_id = H5Dcreate(file_id, "Dataset", H5T_NATIVE_DOUBLE, dspace_id, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);
   H5Dwrite(dset_id, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, data);
 
   // Close resources
   H5Dclose(dset_id);
   H5Sclose(dspace_id);
   H5Fclose(file_id);
   H5Pclose(plist_id);
 
   MPI_Finalize();
   return 0;
}
The H5Pset_fapl_mpio call is unnecessary in parallel I/O.

The dataset creation and writing must be done with individual file access property lists for each process.

The code correctly initializes the HDF5 file for parallel I/O.

The code should use H5F_ACC_RDWR instead of H5F_ACC_TRUNC to open the file for parallel I/O.

The file should be created with H5P_DEFAULT as the file access property list for parallel I/O.

Question 20
20
Multiple Choice
In a large-scale simulation running on an HPC cluster, the time spent on I/O operations (reading/writing data) is significant. Which optimization strategy would help reduce I/O bottlenecks?

Write data to disk after every iteration to prevent data loss.

Use a single thread to handle all I/O operations.

Store data in a global variable to minimize I/O operations.

Increase the size of I/O buffers on the master node.

Implement parallel I/O using MPI-IO.

Question 21
21
Multiple Choice
In a distributed-memory environment using ScaLAPACK, you want to solve a system of linear equations Ax=bAx = bAx=b where A is a large, dense matrix distributed across multiple processors. Which ScaLAPACK function would you use, and how would you initialize the required descriptors for the matrix distribution? (Assume MPI environment is already initialized.)

#include <mpi.h>
#include <scalapack.h>
 
int main(int argc, char *argv[]) {
   MPI_Init(&argc, &argv);
 
   int n = 1000; // Matrix size
   int nb = 100; // Block size
   int nprow = 2, npcol = 2; // Processor grid size
 
   // Initialize process grid
   int ictxt, myrow, mycol, info;
   Cblacs_get(0, 0, &ictxt);
   Cblacs_gridinit(&ictxt, "Row", nprow, npcol);
   Cblacs_gridinfo(ictxt, &nprow, &npcol, &myrow, &mycol);
 
   // Initialize matrix descriptors (Assume arrays A, B, and result X are allocated and filled)
   int descA[9], descB[9], descX[9];
   int lda = nb, ldb = nb, ldx = nb;
   int rsrc = 0;
 
   // Initialize matrix descriptors
   descinit(descA, &n, &n, &nb, &nb, &rsrc, &rsrc, &ictxt, &lda, &info);
   descinit(descB, &n, &1, &nb, &1, &rsrc, &rsrc, &ictxt, &ldb, &info);
   descinit(descX, &n, &1, &nb, &1, &rsrc, &rsrc, &ictxt, &ldx, &info);
 
   // Call to ScaLAPACK routine to solve the system
   int ipiv[n];
   pdgesv_(&n, &1, A, &1, &1, descA, ipiv, B, &1, &1, descB, &info);
 
   MPI_Finalize();
   return 0;
}
Descriptors are incorrectly initialized; the block size nb should not be included in descinit.

The function pdgesv_ is correct, and descriptors are correctly initialized.

The processor grid size should match the matrix dimensions, so nprow and npcol are incorrect.

The Cblacs_gridinit should use "Col" instead of "Row" for the correct process grid orientation.

The function pdgesv_ is incorrect for solving linear equations; use pdgetrf_ instead.

Question 22
22
Multiple Choice
Parallel Input/Output Which statement about Parallel Input/Output (I/O) in HPC is TRUE?

It is primarily used for signal processing.

It is used to solve linear equations.

It focuses on visualizing data.

It allows simultaneous reading/writing of data across multiple processors.

It deals with the decomposition of meshes.

Question 23
23
Multiple Choice
During profiling with Valgrind's Massif tool, you observe that memory consumption peaks during a specific phase of your application. Which of the following strategies could help reduce this memory usage?

Reduce the number of processes running concurrently.

Refactor the code to use stack allocation instead of heap allocation.

Split the workload into smaller, more manageable chunks.

Optimize the cache usage with loop tiling.

Implement checkpointing to save memory state periodically.

Question 24
24
Multiple Choice
You are profiling a parallel MPI application using TAU. After running the profiling session, you observe that a significant amount of time is spent in the following code segment:

MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);


What optimization could reduce the time spent in this MPI function?

Reduce the frequency of calling MPI_Reduce.

Replace the reduction operation with point-to-point communication.

Optimize the data structure used for local_sum.

Use MPI_Bcast instead of MPI_Reduce.

Implement non-blocking communication using MPI_Ireduce.

Question 25
25
Multiple Choice
Given a basic matrix multiplication algorithm, which optimization technique would most effectively reduce cache misses on a modern HPC system?

Loop tiling (blocking)

Vectorization

Loop unrolling

Dynamic scheduling

Speculative execution

Question 26
26
Multiple Choice
Analyse the following matrix multiplication code and identify which optimization technique would most effectively improve cache performance:

for (int i = 0; i < n; i++) {
   for (int j = 0; j < n; j++) {
       for (int k = 0; k < n; k++) {
           C[i][j] += A[i][k] * B[k][j];
       }
   }
}
Vectorization

Loop unrolling

Loop fusion

Speculative execution

Loop tiling (blocking)

Question 27
27
Multiple Choice
When choosing a parallel algorithm for HPC, why is scalability a critical consideration?

It ensures that the code is more readable.

It makes the algorithm compatible with multiple programming languages.

It guarantees the lowest possible execution time on a single processor.

It allows the algorithm to utilize increasing numbers of processors effectively.

It simplifies the debugging process.

Question 28
28
Multiple Choice
Graph Algorithms In HPC, graph algorithms can be used to:

Decompose matrices

Process signals

Determine shortest paths in large-scale networks

Render 3D models

Monitor performance

Question 29
29
Multiple Choice
Why is code optimization particularly crucial in HPC environments?

To maximize performance by effectively utilizing hardware resources.

To ensure compatibility with different compilers.

To minimize the number of programming errors.

To make the code easier to read.

To reduce the code size.

Question 30
30
Multiple Choice
When implementing checkpointing in an HPC application, what is the primary trade-off you need to consider when determining the frequency of checkpoints?

The trade-off between checkpoint size and recovery speed

The trade-off between checkpoint consistency and parallel execution efficiency

The trade-off between fault tolerance and computational precision

The trade-off between checkpointing overhead and the potential loss of computation time in the event of a failure

The trade-off between system-level and application-level checkpointing

Question 31
31
Multiple Choice
You are using TAU (Tuning and Analysis Utilities) to profile a parallel application and notice that some processes are spending a significant amount of time in the MPI_Wait function. What does this suggest, and how might you optimize the application?

There is a load imbalance; redistribute work among processes.

The network bandwidth is insufficient; upgrade the network infrastructure.

The application has excessive I/O operations; reduce I/O frequency.

The processes are not properly synchronized; implement a better locking mechanism.

The application has too many MPI barriers; reduce the number of barriers.

Question 32
32
Multiple Choice
Which role does machine learning play in optimizing I/O operations in HPC environments?

Compressing data to save storage space.

Automatically scaling storage capacity based on usage.

Predicting future data access patterns to optimize caching and data placement.

Encrypting data before it is stored.

Reducing the need for metadata management.

Question 33
33
Multiple Choice
Given the following loop intended for parallel execution using OpenMP, identify the issue with the parallelization strategy:

void scale_array(float* array, int n, float scalar) {
   #pragma omp parallel for
   for (int i = 0; i < n; i++) {
       array[i] *= scalar;
   }
}
The use of scalar in the loop might cause incorrect results due to data races.

The loop cannot be parallelized because the array elements depend on each other.

The array should be split into chunks manually for parallel processing.

There is no issue; the loop is correctly parallelized.

The code will result in a race condition.

Question 34
34
Multiple Choice
Which filesystem would you choose for a high-performance computing (HPC) application that requires high concurrency and large-scale data processing, and why?

Ext4 because it is highly reliable for single-machine environments.

NTFS because it is compatible with Windows operating systems.

HDFS because it offers redundancy and parallel data access.

NFS because it is simple to implement and widely supported.

Lustre because it is optimized for high concurrency and massive data throughput.

Question 35
35
Multiple Choice
Linear Algebra in HPC Real-World Scenario Imagine you are working on a weather simulation program that requires solving a large system of linear equations. Which tool, known for providing routines to solve systems of linear equations, would you consider using?

Linear Algebra Package (LAPACK) - Provides routines for solving systems of linear equations, eigenvalue problems, and matrix inversion.

VTK - Used for 3D data rendering and visualization.

Parallel Boost Graph Library - Designed for large-scale graph operations.

Basic Linear Algebra Subprograms (BLAS) - Focuses on basic vector and matrix operations.

METIS - Used for decomposing graphs/meshes for parallel computing.

Question 36
36
Multiple Choice
Practical Application of HDF5. Consider you are working on a climate model that generates large datasets with multiple variables (e.g., temperature, pressure) that need to be stored efficiently. Which features of the HDF5 library make it particularly suitable for this task?

HDF5 is designed specifically for image processing and does not support multidimensional data arrays.

HDF5 only supports serial I/O operations, making it unsuitable for large-scale data handling.

HDF5 is limited to handling small datasets and is not optimized for parallel processing.

HDF5 does not provide tools for metadata management, making it challenging to handle large datasets.

HDF5 offers a hierarchical data model that can manage complex data relationships and allows parallel I/O operations, ensuring efficient data storage and retrieval.

Question 37
37
Multiple Choice
Mesh Decomposition in Aerospace Engineering An aerospace engineer is working on a simulation of airflow over an aircraft wing. To ensure parallel computing efficiency, the engineer needs to decompose the computational mesh around the wing. Which tool specializes in this?

ELPA - Performs matrix operations.

SuperLU - Solves systems of linear equations.

VTK - Used for 3D data visualization.

Boost MPI - C++ interface for Message Passing Interface.

METIS - Efficiently decomposes graphs and meshes for parallel computing.

Question 38
38
Multiple Choice
Parallel Input/Output in Astrophysics An astrophysicist is dealing with vast datasets from cosmic simulations. The data needs to be read and written efficiently in a parallel manner across multiple nodes of a supercomputer. Which tool is designed for this specific task?

Trilinos - Aims at solving PDEs.

METIS - Decomposes meshes for parallel computing.

HDF5 - Enables efficient storage and retrieval of vast datasets in parallel.

PAPI - Provides performance metrics from hardware counters.

SuperLU - Solves systems of linear equations.

Question 39
39
Multiple Choice
In an image processing application running on an HPC system, the memory bandwidth is the primary bottleneck. Which technique would most likely improve performance?

Storing images as linked lists for faster access.

Using higher precision for image pixel values.

Increasing the size of the images being processed.

Using loop tiling and prefetching to optimize data access patterns.

Converting the images to grayscale before processing.

Question 40
40
Multiple Choice
What are the main three libraries from SuperLU?

Multithreaded SuperLU designed for SMP architectures.

Sequential SuperLU, designed for sequential execution on processors with cache-based memory hierarchies.

Distributed SuperLU is designed for distributed-memory architectures. 

AccSuperLU used only for architectures with Accelerators (GPUS)

Question 41
41
Multiple Choice
In a parallel application, you use Scalasca to analyze MPI communication patterns. The tool reports a high percentage of time spent in MPI_Barrier calls. What does this suggest, and how could you optimize the application?

Indicates excessive branching; simplify conditional statements.

Indicates inefficient thread management; reduce the number of threads.

Indicates excessive data movement; reduce the number of MPI_Barrier calls.

Indicates poor memory access patterns; optimize data layout.

Indicates load imbalance; improve workload distribution among processes.

Question 42
42
Multiple Choice
What is the most important library for distributed-memory architectures?

OpenMP

Pthreads

VTK

MPI

Question 43
43
Multiple Choice
Question: Given the following code snippet using OpenMP to parallelize a matrix-vector multiplication, what will be the expected output for the resulting vector y?

#include <omp.h>
#include <stdio.h>
 
int main() {
   int n = 3;
   double A[3][3] = {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}};
   double x[3] = {1, 2, 3};
   double y[3] = {0};
 
   #pragma omp parallel for
   for (int i = 0; i < n; i++) {
       for (int j = 0; j < n; j++) {
           y[i] += A[i][j] * x[j];
       }
   }
 
   printf("Resulting vector y:\n");
   for (int i = 0; i < n; ++i) {
       printf("%f ", y[i]);
   }
   printf("\n");
 
   return 0;
}
14.00 32.00 50.00

30.00 36.00 42.00

14.00 28.00 42.00

6.00 15.00 24.00

12.00 30.00 48.00

Question 44
44
Multiple Choice
Partial Differential Equations In the context of High Performance Computing, why are Partial Differential Equations (PDEs) significant?

They are used to visualize complex datasets.

They are fundamental in graph algorithms.

They help in mesh decomposition.

They represent simple algebraic problems.

They model many real-world problems like fluid dynamics.

Question 45
45
Multiple Choice
You are integrating checkpointing into a large-scale HPC application using DMTCP (Distributed MultiThreaded CheckPointing). What are the key considerations to ensure minimal performance impact while maintaining robust fault tolerance?

The number of threads used, the file system type, and the memory usage of the application.

The checkpoint interval, the size of checkpoint files, and the overhead of network communication.

The use of hardware counters, the clock speed of the CPU, and the size of the application binaries.

The number of nodes involved, the frequency of synchronization, and the CPU clock speed.

The precision of floating-point operations, the cache size, and the number of I/O operations.

Question 46
46
Multiple Choice
Weather forecasting models on HPC systems often involve large-scale matrix operations. What data structure choice would most efficiently utilize the system's memory hierarchy?

Using dynamic memory allocation for each matrix element.

Using arrays with contiguous memory allocation.

Using hash tables for matrix storage.

Using linked lists to store matrix data.

Storing data on disk and accessing it as needed.

Question 47
47
Multiple Choice
Given the following C code using the BLAS library, what is the correct output for the resulting matrix C?

#include <stdio.h>
#include <cblas.h>
 
int main() {
   int m = 2, n = 3, k = 2;
   double A[6] = {1, 2, 3, 4, 5, 6}; // 2x3 matrix
   double B[6] = {7, 8, 9, 10, 11, 12}; // 3x2 matrix
   double C[4] = {0}; // 2x2 result matrix
 
   // Perform C = A * B using DGEMM
   cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans,
               m, k, n, 1.0, A, n, B, k, 0.0, C, k);
 
   printf("Result matrix C:\n");
   for (int i = 0; i < 4; ++i) {
       printf("%f ", C[i]);
       if ((i+1) % k == 0) printf("\n");
   }
   return 0;
}
19.00 22.00

43.00 50.00

67.00 76.00

150.00 165.00

58.00 64.00

139.00 154.00

34.00 42.00

85.00 100.00

37.00 42.00

85.00 96.00

Question 48
48
Multiple Choice
Which profiling technique is most suitable for identifying performance bottlenecks in parallel applications?

Call graph profiling

Tracing

Sampling

Instrumentation

Memory profiling

Question 49
49
Multiple Choice
While analysing performance data using Intel VTune Amplifier, you identify that a certain function has high L3 cache misses. What might be causing this, and how could you optimize the function to improve cache performance?

Excessive branching in the function; optimize branch prediction.

Insufficient parallelism; increase the number of threads.

The function is too small; consider inlining it.

Frequent I/O operations within the function; reduce I/O frequency.

Large data structures not fitting into the cache; implement data blocking or partitioning.

Question 50
50
Multiple Choice
Why are high-speed interconnects like InfiniBand crucial for parallel filesystems in HPC environments?

They simplify the management of metadata across nodes.

They increase the storage capacity of the filesystem.

They provide better data redundancy.

They facilitate fast data transfers between compute nodes and storage systems, reducing latency.

They reduce the cost of storage devices.